{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PID\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.interpolate import make_interp_spline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define PID weights\n",
    "\n",
    "PID controller minimizes error by adjusting a control variable (eg power supplied) to a new value determined by a weighted sum of present (P), past (I), and future (D) error values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 1.6 # weight current errors more\n",
    "I = 4\n",
    "D = 0.000000001 # ignore future potential errors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 50 # number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid = PID.PID(1.00, 1.00, 1.83)\n",
    "\n",
    "pid.SetPoint=0.0\n",
    "pid.setSampleTime(0.01)\n",
    "\n",
    "END = L\n",
    "feedback = 0\n",
    "\n",
    "feedback_list = []\n",
    "time_list = []\n",
    "setpoint_list = []\n",
    "\n",
    "p_term_list = []\n",
    "i_term_list = []\n",
    "d_term_list = []\n",
    "\n",
    "for i in range(1, END):\n",
    "    pid.update(feedback)\n",
    "    \n",
    "    p_term_list.append(pid.PTerm)\n",
    "    i_term_list.append(pid.ITerm)\n",
    "    d_term_list.append(pid.DTerm)\n",
    "    \n",
    "    output = pid.output\n",
    "    if pid.SetPoint > 0:\n",
    "        feedback += (output - (1/i))\n",
    "    if i>9:\n",
    "        pid.SetPoint = 1\n",
    "    time.sleep(0.02)\n",
    "\n",
    "    feedback_list.append(feedback)\n",
    "    setpoint_list.append(pid.SetPoint)\n",
    "    time_list.append(i)\n",
    "\n",
    "time_sm = np.array(time_list)\n",
    "time_smooth = np.linspace(time_sm.min(), time_sm.max(), 300) \n",
    "feedback_smooth = make_interp_spline(time_list, feedback_list)(time_smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how quickly does it converge?\n",
    "\n",
    "green is desired value; blue is actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 1.5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(time_smooth, feedback_smooth)\n",
    "plt.plot(time_list, setpoint_list)\n",
    "plt.xlim((0, L))\n",
    "plt.ylim((min(feedback_list)-0.5, max(feedback_list)+0.5))\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('PID (PV)')\n",
    "plt.title('TEST PID')\n",
    "\n",
    "plt.ylim((1-0.5, 1+0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가설 1 : PTerm, ITerm, DTerm의 변화가 크지 않다?\n",
    "\n",
    "각 Term의 차이가 크지 않다면(차이가 거의 없다면), 한번 trial한 데이터로 최적의 P, I, D값을 뽑을 수 있지 않을까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = [(1.5,5,0),(1.3,3,0.001),(1.2,2,0.0001)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,15))\n",
    "plt.xlabel('time (s)')\n",
    "\n",
    "ax_pid = plt.subplot(4,1,1)\n",
    "ax_p = plt.subplot(4,1,2)\n",
    "ax_i = plt.subplot(4,1,3)\n",
    "ax_d = plt.subplot(4,1,4)\n",
    "\n",
    "ax_pid.set_title('PID output')\n",
    "ax_p.set_title('PTerm')\n",
    "ax_i.set_title('ITerm')\n",
    "ax_d.set_title('DTerm')\n",
    "\n",
    "# SetPoint Plot Setting\n",
    "ax_pid.plot(time_list, setpoint_list)\n",
    "\n",
    "\n",
    "for case in cases:\n",
    "    pid = PID.PID(*case)\n",
    "\n",
    "    pid.SetPoint=0.0\n",
    "    pid.setSampleTime(0.01)\n",
    "\n",
    "    END = L\n",
    "    feedback = 0\n",
    "\n",
    "    feedback_list = []\n",
    "    time_list = []\n",
    "    setpoint_list = []\n",
    "\n",
    "    p_term_list = []\n",
    "    i_term_list = []\n",
    "    d_term_list = []\n",
    "\n",
    "    for i in range(1, END):\n",
    "        pid.update(feedback)\n",
    "\n",
    "        p_term_list.append(pid.PTerm)\n",
    "        i_term_list.append(pid.ITerm)\n",
    "        d_term_list.append(pid.DTerm)\n",
    "\n",
    "        output = pid.output\n",
    "        if pid.SetPoint > 0:\n",
    "            feedback += (output - (1/i))\n",
    "        if i>9:\n",
    "            pid.SetPoint = 1\n",
    "        time.sleep(0.02)\n",
    "\n",
    "        feedback_list.append(feedback)\n",
    "        setpoint_list.append(pid.SetPoint)\n",
    "        time_list.append(i)\n",
    "\n",
    "    time_sm = np.array(time_list)\n",
    "    time_smooth = np.linspace(time_sm.min(), time_sm.max(), 300) \n",
    "    feedback_smooth = make_interp_spline(time_list, feedback_list)(time_smooth)\n",
    "    \n",
    "    ax_pid.plot(time_smooth, feedback_smooth, label=f'{case}')\n",
    "    ax_pid.legend()\n",
    "    \n",
    "    ax_p.plot(time_list, p_term_list, label=f'{case}')\n",
    "    ax_p.legend()\n",
    "    \n",
    "    ax_i.plot(time_list, i_term_list, label=f'{case}')\n",
    "    ax_i.legend()\n",
    "    \n",
    "    ax_d.plot(time_list, d_term_list, label=f'{case}')\n",
    "    ax_d.legend()\n",
    "    \n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "차이가 많이 난다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가설2 : PID 제어 도중에 p, i, d값을 업데이트하면 어떻게 될까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "P = np.array([0 + p*0.02 for p in range(100)]) # weight current errors more\n",
    "I = np.array([0 + i*0.02 for i in range(100)])\n",
    "D = np.array([0.0000005 + d*0.00000001 for d in range(100)]) # ignore future potential errors \n",
    "L = 100 # number of iterations\n",
    "\n",
    "pid = PID.PID()\n",
    "\n",
    "pid.SetPoint=0.0\n",
    "pid.setSampleTime(0.01)\n",
    "\n",
    "END = L\n",
    "feedback = 0\n",
    "\n",
    "feedback_list = []\n",
    "time_list = []\n",
    "setpoint_list = []\n",
    "\n",
    "for i in range(1, END):\n",
    "    \n",
    "    '''\n",
    "    강화학습 알고리즘이 들어올 곳\n",
    "    '''\n",
    "    pid.Kp = P[i]\n",
    "    pid.Ki = I[i]\n",
    "    pid.Kd = D[i]\n",
    "    \n",
    "\n",
    "    pid.update(feedback)\n",
    "    setpoint_list.append(pid.SetPoint)\n",
    "    \n",
    "    output = pid.output\n",
    "#     if pid.SetPoint = 0:\n",
    "    feedback += output\n",
    "    if 9<i<50:\n",
    "        pid.SetPoint = 1\n",
    "    elif 50<i<90:\n",
    "        pid.SetPoint = -1\n",
    "    else:\n",
    "        pid.SetPoint = 0\n",
    "        \n",
    "    time.sleep(0.03)\n",
    "\n",
    "    feedback_list.append(feedback)\n",
    "    time_list.append(i)    \n",
    "\n",
    "plt.plot(time_list, feedback_list)\n",
    "plt.plot(time_list, setpoint_list)\n",
    "plt.xlim((0, L))\n",
    "plt.ylim((min(feedback_list)-0.5, max(feedback_list)+0.5))\n",
    "plt.xlabel('time (s)')\n",
    "plt.ylabel('PID (PV)')\n",
    "plt.title('TEST PID')\n",
    "\n",
    "plt.ylim((-1-0.5, 1+0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가설3 : PID제어 도중 PID-tunning RL 도입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QAC\n",
    "import sys\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C가 제대로 학습되는지 확인 - CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class A2CAgent:\n",
    "#     def __init__(self, state_size, action_size):\n",
    "#         self.render = False\n",
    "#         self.load_model = False\n",
    "#         # 상태와 행동의 크기 정의\n",
    "#         self.state_size = state_size\n",
    "#         self.action_size = action_size\n",
    "#         self.value_size = 1\n",
    "#         self.update_bound = 10\n",
    "\n",
    "#         # 액터-크리틱 하이퍼파라미터\n",
    "#         self.discount_factor = 0.99\n",
    "#         self.actor_lr = 0.001\n",
    "#         self.critic_lr = 0.005\n",
    "\n",
    "#         # 정책신경망과 가치신경망 생성\n",
    "#         self.actor = self.build_actor()\n",
    "#         self.critic = self.build_critic()\n",
    "#         self.actor_optimizer = tf.keras.optimizers.Adam()\n",
    "#         self.critic_optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "#         if self.load_model:\n",
    "#             self.actor.load_weights(\"./save_model/actor_trained.h5\")\n",
    "#             self.critic.load_weights(\"./save_model/critic_trained.h5\")\n",
    "\n",
    "#     # actor: 상태를 받아 각 행동의 확률을 계산\n",
    "#     def build_actor(self):\n",
    "#         actor = Sequential()\n",
    "#         actor.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
    "#                         kernel_initializer='he_uniform'))\n",
    "#         actor.add(Dense(self.action_size, activation='softmax',\n",
    "#                         kernel_initializer='he_uniform'))\n",
    "#         return actor\n",
    "\n",
    "#     # critic: 상태를 받아서 상태의 가치를 계산\n",
    "#     def build_critic(self):\n",
    "#         critic = Sequential()\n",
    "#         critic.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
    "#                          kernel_initializer='he_uniform'))\n",
    "#         critic.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
    "#                          kernel_initializer='he_uniform'))\n",
    "#         critic.add(Dense(self.value_size, activation='linear',\n",
    "#                          kernel_initializer='he_uniform'))\n",
    "#         return critic\n",
    "    \n",
    "#     def get_action(self, state):\n",
    "#         policy = self.actor.predict(state, batch_size=1).flatten()\n",
    "#         return np.random.choice(self.action_size, 1, p=policy)[0]\n",
    "\n",
    "#     # 정책신경망을 업데이트하는 함수\n",
    "#     def train_actor(self, action, state, advantage):\n",
    "#         with tf.GradientTape() as t:\n",
    "#             output = self.actor(state)\n",
    "#             output_prob = K.sum(action * output, axis=1)\n",
    "#             loss = -K.sum(K.log(output_prob) * advantage)\n",
    "#         g_theta = t.gradient(loss, self.actor.trainable_weights)\n",
    "#         grads = zip(g_theta, self.actor.trainable_weights)\n",
    "#         self.actor_optimizer.apply_gradients(grads)\n",
    "\n",
    "#     # 가치신경망을 업데이트하는 함수\n",
    "#     def train_critic(self, state, target):\n",
    "#         with tf.GradientTape() as t:\n",
    "#             output = self.critic(state)\n",
    "#             loss = K.mean(K.square(target - output))\n",
    "#         g_omega = t.gradient(loss, self.critic.trainable_weights)\n",
    "#         grads = zip(g_omega, self.critic.trainable_weights)\n",
    "#         self.critic_optimizer.apply_gradients(grads)\n",
    "\n",
    "#     # 각 타임스텝마다 정책신경망과 가치신경망을 업데이트\n",
    "#     def train_model(self, state, action, reward, next_state, done):\n",
    "#         value = self.critic(state)[0]\n",
    "#         next_value = self.critic(next_state)[0]\n",
    "        \n",
    "#         act = np.zeros([1, self.action_size])\n",
    "#         act[0][action] = 1\n",
    "\n",
    "#         # 벨만 기대 방정식를 이용한 어드벤티지와 업데이트 타깃\n",
    "#         advantage = reward - value + (1 - done)*(self.discount_factor * next_value)\n",
    "#         target = reward + (1 - done)*(self.discount_factor * next_value)\n",
    "        \n",
    "#         self.train_actor(act, state, advantage)\n",
    "#         self.train_critic(state, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import gym\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # CartPole-v1 환경, 최대 타임스텝 수가 500\n",
    "#     env = gym.make('CartPole-v1')\n",
    "#     # 환경으로부터 상태와 행동의 크기를 받아옴\n",
    "#     state_size = env.observation_space.shape[0]\n",
    "#     action_size = env.action_space.n\n",
    "\n",
    "#     # 액터-크리틱(A2C) 에이전트 생성\n",
    "#     agent = A2CAgent(state_size, action_size)\n",
    "\n",
    "#     scores, episodes = [], []\n",
    "\n",
    "#     for e in range(400):\n",
    "#         done = False\n",
    "#         score = 0\n",
    "#         state = env.reset()\n",
    "#         state = np.reshape(state, [1, state_size])\n",
    "\n",
    "#         while not done:\n",
    "#             if agent.render:\n",
    "#                 env.render()\n",
    "\n",
    "#             action = agent.get_action(state)\n",
    "#             next_state, reward, done, info = env.step(action)\n",
    "#             next_state = np.reshape(next_state, [1, state_size])\n",
    "#             # 에피소드가 중간에 끝나면 -100 보상\n",
    "#             reward = reward if not done or score == 499 else -100\n",
    "\n",
    "#             agent.train_model(state, action, reward, next_state, done)\n",
    "\n",
    "#             score += reward\n",
    "#             state = next_state\n",
    "\n",
    "#             if done:\n",
    "#                 # 에피소드마다 학습 결과 출력\n",
    "#                 score = score if score == 500.0 else score + 100\n",
    "#                 scores.append(score)\n",
    "#                 episodes.append(e)\n",
    "#                 plt.plot(episodes, scores, 'b')\n",
    "#                 plt.savefig(\"./save_graph/cartpole_a2c.png\")\n",
    "#                 print(\"episode:\", e, \"  score:\", score)\n",
    "\n",
    "#                 # 이전 10개 에피소드의 점수 평균이 490보다 크면 학습 중단# import gym\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # CartPole-v1 환경, 최대 타임스텝 수가 500\n",
    "#     env = gym.make('CartPole-v1')\n",
    "#     # 환경으로부터 상태와 행동의 크기를 받아옴\n",
    "#     state_size = env.observation_space.shape[0]\n",
    "#     action_size = env.action_space.n\n",
    "\n",
    "#     # 액터-크리틱(A2C) 에이전트 생성\n",
    "#     agent = A2CAgent(state_size, action_size)\n",
    "\n",
    "#     scores, episodes = [], []\n",
    "\n",
    "#     for e in range(400):\n",
    "#         done = False\n",
    "#         score = 0\n",
    "#         state = env.reset()\n",
    "#         state = np.reshape(state, [1, state_size])\n",
    "\n",
    "#         while not done:\n",
    "#             if agent.render:\n",
    "#                 env.render()\n",
    "\n",
    "#             action = agent.get_action(state)\n",
    "#             next_state, reward, done, info = env.step(action)\n",
    "#             next_state = np.reshape(next_state, [1, state_size])\n",
    "#             # 에피소드가 중간에 끝나면 -100 보상\n",
    "#             reward = reward if not done or score == 499 else -100\n",
    "\n",
    "#             agent.train_model(state, action, reward, next_state, done)\n",
    "\n",
    "#             score += reward\n",
    "#             state = next_state\n",
    "\n",
    "#             if done:\n",
    "#                 # 에피소드마다 학습 결과 출력\n",
    "#                 score = score if score == 500.0 else score + 100\n",
    "#                 scores.append(score)\n",
    "#                 episodes.append(e)\n",
    "#                 plt.plot(episodes, scores, 'b')\n",
    "#                 plt.savefig(\"./save_graph/cartpole_a2c.png\")\n",
    "#                 print(\"episode:\", e, \"  score:\", score)\n",
    "\n",
    "#                 # 이전 10개 에피소드의 점수 평균이 490보다 크면 학습 중단\n",
    "#                 if np.mean(scores[-min(10, len(scores)):]) > 490:\n",
    "# #                     agent.actor.save_weights(\"./save_model/cartpole_actor.h5\")\n",
    "# #                     agent.critic.save_weights(\n",
    "# #                         \"./save_model/cartpole_critic.h5\")\n",
    "#                     sys.exit()\n",
    "\n",
    "#     env.close()\n",
    "#                 if np.mean(scores[-min(10, len(scores)):]) > 490:\n",
    "# #                     agent.actor.save_weights(\"./save_model/cartpole_actor.h5\")\n",
    "# #                     agent.critic.save_weights(\n",
    "# #                         \"./save_model/cartpole_critic.h5\")\n",
    "#                     sys.exit()\n",
    "\n",
    "#     env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A2C학습이 잘 되는 것 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PID Tunning을 위한 A2C 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CPIDTunner:\n",
    "    def __init__(self, state_size, action_size, load_model=False):\n",
    "        self.load_model = load_model\n",
    "        # 상태와 행동의 크기 정의\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.value_size = 1\n",
    "        self.grad_bound = 0.\n",
    "        \n",
    "        self.std_bound = [1e-2, 1.0]\n",
    "\n",
    "        # 액터-크리틱 하이퍼파라미터\n",
    "        self.discount_factor = 0.99\n",
    "        self.actor_lr = 0.001\n",
    "        self.critic_lr = 0.005\n",
    "\n",
    "        # 정책신경망과 가치신경망 생성\n",
    "        self.actor = self.build_actor()\n",
    "        self.critic = self.build_critic()\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam()\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.actor.load_weights(\"./save_model/actor_trained.h5\")\n",
    "            self.critic.load_weights(\"./save_model/critic_trained.h5\")\n",
    "\n",
    "    # actor: 상태를 받아 각 행동의 확률을 계산\n",
    "    def build_actor(self):\n",
    "        input_state = tf.keras.Input((self.state_size,))\n",
    "        d1 = tf.keras.layers.Dense(24, activation='relu')(input_state)\n",
    "        out_mu = tf.keras.layers.Dense(self.action_size, activation='tanh')(d1)\n",
    "        out_std = tf.keras.layers.Dense(self.action_size, activation='softplus')(d1)\n",
    "        actor =  tf.keras.Model(input_state, [out_mu, out_std])\n",
    "        return actor\n",
    "\n",
    "    # critic: 상태를 받아서 상태의 가치를 계산\n",
    "    def build_critic(self):\n",
    "        input_state = tf.keras.Input((self.state_size,))\n",
    "        d1 = tf.keras.layers.Dense(24, activation='relu')(input_state)\n",
    "        d2 = tf.keras.layers.Dense(24, activation='relu')(d1)\n",
    "        output = tf.keras.layers.Dense(1, activation='tanh')(d2)\n",
    "        critic = tf.keras.Model(input_state, output)\n",
    "        return critic\n",
    "\n",
    "    # log_policy pdf\n",
    "    def log_pdf(self, mu, std, action):\n",
    "        std = tf.clip_by_value(std, self.std_bound[0], self.std_bound[1])\n",
    "        var = std**2\n",
    "        log_policy_pdf = -0.5 * (action - mu) ** 2 / var - 0.5 * tf.math.log(var * 2 * np.pi)\n",
    "        return tf.reduce_sum(log_policy_pdf, 1, keepdims=True)\n",
    "    \n",
    "    # 정책신경망의 출력을 받아 확률적으로 행동을 선택\n",
    "    def get_action(self, state):\n",
    "        mu, std = self.actor(np.reshape(state, [1, self.state_size]))\n",
    "        mu = mu[0]\n",
    "        std = std[0]\n",
    "\n",
    "        std = tf.clip_by_value(std, self.std_bound[0], self.std_bound[1])\n",
    "        action = np.random.normal(mu, std, size=self.action_size)\n",
    "        return action\n",
    "\n",
    "    # 정책신경망을 업데이트하는 함수\n",
    "    def train_actor(self, action, state, advantage):\n",
    "        with tf.GradientTape() as t:\n",
    "            mu_a, std_a = self.actor(state)\n",
    "            log_policy_pdf = self.log_pdf(mu_a, std_a, action)\n",
    "            loss = -K.sum(log_policy_pdf * advantage)\n",
    "        g_theta = t.gradient(loss, self.actor.trainable_weights)\n",
    "        grads = zip(g_theta, self.actor.trainable_weights)\n",
    "        grads = [(tf.clip_by_value(grad, -self.grad_bound, self.grad_bound), var) for grad, var in grads]\n",
    "        self.actor_optimizer.apply_gradients(grads)\n",
    "\n",
    "    # 가치신경망을 업데이트하는 함수\n",
    "    def train_critic(self, state, target):\n",
    "        with tf.GradientTape() as t:\n",
    "            output = self.critic(state)\n",
    "            loss = K.mean(K.square(target - output))\n",
    "        g_omega = t.gradient(loss, self.critic.trainable_weights)\n",
    "        grads = zip(g_omega, self.critic.trainable_weights)\n",
    "        grads = [(tf.clip_by_value(grad, -self.grad_bound, self.grad_bound), var) for grad, var in grads]\n",
    "        self.critic_optimizer.apply_gradients(grads)\n",
    "\n",
    "    # 각 타임스텝마다 정책신경망과 가치신경망을 업데이트\n",
    "    def train_model(self, state, action, reward, next_state, done):\n",
    "        value = self.critic(state)[0]\n",
    "        next_value = self.critic(next_state)[0]\n",
    "\n",
    "        # 벨만 기대 방정식를 이용한 어드벤티지와 업데이트 타깃\n",
    "        advantage = reward - value + (1 - done)*(self.discount_factor * next_value)\n",
    "        target = reward + (1 - done)*(self.discount_factor * next_value)\n",
    "        \n",
    "        self.train_actor(action, state, advantage)\n",
    "        self.train_critic(state, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample 환경 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PIDsampleEnv(PID.PID):\n",
    "    \n",
    "    def __init__(self, P=0.2, I=0.0, D=0.0, set_point=1):\n",
    "        super().__init__(self)\n",
    "        \n",
    "        self.Kp = P\n",
    "        self.Ki = I\n",
    "        self.Kd = D\n",
    "    \n",
    "        self.set_point = set_point\n",
    "        self.end = 80\n",
    "        \n",
    "    def reset(self):\n",
    "        self.clear()\n",
    "        return np.zeros(self.end)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "        max_error = 0\n",
    "        feedback = 0\n",
    "        done = False\n",
    "        \n",
    "        self.Kp = action[0]\n",
    "        self.Ki = action[1]\n",
    "        self.Kd = action[2]\n",
    "        \n",
    "        next_state = []\n",
    "        \n",
    "        for i in range(1, self.end+1):\n",
    "            self.update(feedback)\n",
    "            output = self.output\n",
    "            \n",
    "            if self.SetPoint > 0:\n",
    "                feedback = output\n",
    "                next_state.append(feedback)\n",
    "            if 10<=i<60:\n",
    "                self.SetPoint = self.set_point\n",
    "            else:\n",
    "                self.SetPoint = 0\n",
    "            \n",
    "            error = self.last_error\n",
    "            if abs(error) > max_error:\n",
    "                max_error = abs(error)\n",
    "            \n",
    "        if max_error < (self.SetPoint)*0.1:\n",
    "            done = True\n",
    "        \n",
    "        return next_state, -max_error, done\n",
    "    \n",
    "    def plot(self):\n",
    "        \n",
    "        feedback = 0.0\n",
    "\n",
    "        feedback_list = []\n",
    "        time_list = []\n",
    "        setpoint_list = []\n",
    "\n",
    "        for i in range(1, self.end+1):\n",
    "            \n",
    "            self.update(feedback)\n",
    "            output = self.output\n",
    "            \n",
    "            setpoint_list.append(self.SetPoint)\n",
    "\n",
    "            if self.SetPoint > 0:\n",
    "                feedback += output\n",
    "            \n",
    "            if 10<=i<60:\n",
    "                self.SetPoint = self.set_point\n",
    "            else:\n",
    "                self.SetPoint = 0.0\n",
    "            \n",
    "            feedback_list.append(feedback)    \n",
    "            time_list.append(i)\n",
    "\n",
    "        plt.plot(time_list, feedback_list)\n",
    "        plt.plot(time_list, setpoint_list)\n",
    "        plt.xlim((0, L))\n",
    "        plt.ylim((min(feedback_list)-0.5, max(feedback_list)+0.5))\n",
    "        plt.xlabel('time (s)')\n",
    "        plt.ylabel('PID (PV)')\n",
    "        plt.title('TEST PID')\n",
    "\n",
    "        plt.ylim((0-0.5, 1+0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PIDsampleEnv(set_point=1)\n",
    "\n",
    "state_size = 80\n",
    "action_size = 3\n",
    "\n",
    "# 액터-크리틱(A2C) 에이전트 생성\n",
    "agent = A2CPIDTunner(state_size, action_size, load_model=False)\n",
    "\n",
    "scores, episodes = [], []\n",
    "\n",
    "\n",
    "for e in range(400):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        # 에피소드가 중간에 끝나면 -100 보상\n",
    "        reward = reward if not done or score == 499 else -100\n",
    "\n",
    "        agent.train_model(state, action, reward, next_state, done)\n",
    "\n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            # 에피소드마다 학습 결과 출력\n",
    "            score = score if score == 500.0 else score + 100\n",
    "            scores.append(score)\n",
    "            episodes.append(e)\n",
    "            plt.plot(episodes, scores, 'b')\n",
    "            plt.savefig(\"./save_graph/cartpole_a2c.png\")\n",
    "            print(\"episode:\", e, \"  score:\", score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "activation\n",
    "\n",
    "gradient clipping\n",
    "\n",
    "p, i, d modification\n",
    "\n",
    "끊임없이 시도.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
